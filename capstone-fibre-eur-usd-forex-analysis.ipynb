{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9359824,"sourceType":"datasetVersion","datasetId":5674874}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Capstone 4 - \"Fibre\" (EUR/USD) Forex Analysis**","metadata":{}},{"cell_type":"markdown","source":"**Background**","metadata":{}},{"cell_type":"markdown","source":"The EUR/USD (Euro/US Dollar) currency pair is one of the most widely traded pairs in the foreign exchange (forex) market, representing the exchange rate between the Euro, the official currency of the Eurozone, and the US Dollar, the official currency of the United States. The popularity of this pair stems from the fact that it involves the two largest economies in the world, the Eurozone and the United States, making it a key indicator of global economic health and investor sentiment.\n\nThe dataset provided contains financial and trading-related data, including features such as **Trades**, **Gross Profit**, **Gross Loss**, **% Change**, **Profit Factor**, **Winners**, and **Net Profit**. This data represents key performance indicators (KPIs) that are typically used in evaluating the success of trading strategies and financial outcomes over a period of time. The dataset may include both summary statistics for trading strategies and profit metrics, which provide insights into profitability and risks associated with trades.\n\nGiven the financial nature of the dataset, it offers an opportunity to apply machine learning methods to derive valuable insights and predictive capabilities. Predictive modeling can aid in identifying trends, minimizing risks, and optimizing decision-making processes in financial trading strategies.","metadata":{}},{"cell_type":"markdown","source":"**Objective**","metadata":{}},{"cell_type":"markdown","source":"The primary objective of this project is to apply various machine learning methods to analyze and predict financial performance metrics, such as Net Profit, based on features like Trades, Gross Profit, Gross Loss, % Change, and others. Specifically, the focus will be on the following:\n\n1. **Prediction of Financial Outcomes**: Using supervised learning techniques such as **Regression Analysis** (e.g., Linear Regression, K-Nearest Neighbors, Neural Networks) to predict key financial metrics like Net Profit based on historical data.\n\n2. **Exploration of Relationships Between Variables**: Utilize visualizations such as **pairplots** to explore the relationships between features and their impact on profit outcomes. This will help identify key drivers of profitability in the dataset.\n\n3. **Improvement of Financial Forecasting**: Explore advanced machine learning models, including **Neural Networks**, to improve the accuracy of predictions, uncover hidden patterns, and provide actionable insights for optimizing trading strategies.\n\nThe overall goal is to enhance predictive accuracy, which can lead to better decision-making, risk management, and profitability in trading and financial strategy optimization.","metadata":{}},{"cell_type":"markdown","source":"**Origin of Data**","metadata":{}},{"cell_type":"markdown","source":"Dataset is downloaded from www.fxblue.com which I have collected the data between 8 Oct 2023 to 4 Aug 2024.\n\nAll the \"Magic No.\" is an individual Expert Advisor (EA).\r\n\r\nDataset has the records of 200 rows with 40 columns.","metadata":{}},{"cell_type":"code","source":"# Importing basic libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading the Data**","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndata = pd.read_csv('/kaggle/input/forex2023/Forex.csv')\n\n# Display the first few rows of the dataset\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EDA and Data Preparation**","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data,hue='Net profit')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrdf = data.corr()\nplt.figure(figsize=(40,30))\nsns.heatmap(corrdf, annot=True, cmap=\"coolwarm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove some of the columns**","metadata":{}},{"cell_type":"code","source":"# Define the target variable (e.g., 'Net profit')\ntarget = 'Net profit'\n\n# Select features (excluding the target variable and any non-numeric columns)\nfeatures = data.drop(columns=['Average loser length (hours)', 'Average winner length (hours)', 'Avg pips per trade', 'Net pips', 'Magic', 'Opening balance', 'Closing balance', 'Avg lots','Valley (cash)', 'Valley (pips)','Gross profit', 'Gross loss', '% change', 'Avg win', 'Avg loss', 'Longest (hours)', 'Shortest (hours)', 'Lots traded', 'Consec winners', 'Consec losses', 'Consec profit', 'Consec loss','Winners', 'Losers'])\n\n# Convert all features to numeric if needed (useful if there are categorical variables)\nfeatures = features.apply(pd.to_numeric, errors='coerce')\n\n# Drop any rows with missing values (optional, based on your data)\nfeatures = features.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(features,hue='Net profit')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrdf = features.corr()\nplt.figure(figsize=(30,20))\nsns.heatmap(corrdf, annot=True, cmap=\"coolwarm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, data[target], test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Linear Regression Model**","metadata":{}},{"cell_type":"code","source":"# Initialize the Linear Regression model\nmodel = LinearRegression()\n\n# Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Get the model's coefficients\nprint(\"Coefficients:\", model.coef_)\nprint(\"Intercept:\", model.intercept_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred_lr = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate evaluation metrics\nmae = mean_absolute_error(y_test, y_pred_lr)\nmse = mean_squared_error(y_test, y_pred_lr)\nr2 = r2_score(y_test, y_pred_lr)\n\n# Print evaluation results\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R² Score: {r2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coefficients of the model\ncoefficients = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': model.coef_})\n\n# Display the coefficients\nprint(coefficients)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting actual vs predicted values\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=y_test, y=y_pred_lr, color='blue')\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression Model - Actual vs Predicted Values')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision Tree and Random Forest Regressor**","metadata":{}},{"cell_type":"code","source":"# Initialize the Decision Tree Regressor\ndt_model = DecisionTreeRegressor(random_state=42)\n\n# Train the model on the training data\ndt_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_dt = dt_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate evaluation metrics for Decision Tree\nmae_dt = mean_absolute_error(y_test, y_pred_dt)\nmse_dt = mean_squared_error(y_test, y_pred_dt)\nr2_dt = r2_score(y_test, y_pred_dt)\n\n# Print evaluation results\nprint(f\"Decision Tree - Mean Absolute Error (MAE): {mae_dt}\")\nprint(f\"Decision Tree - Mean Squared Error (MSE): {mse_dt}\")\nprint(f\"Decision Tree - R² Score: {r2_dt}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the Random Forest Regressor\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model on the training data\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_rf = rf_model.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate evaluation metrics for Random Forest\nmae_rf = mean_absolute_error(y_test, y_pred_rf)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\n\n# Print evaluation results\nprint(f\"Random Forest - Mean Absolute Error (MAE): {mae_rf}\")\nprint(f\"Random Forest - Mean Squared Error (MSE): {mse_rf}\")\nprint(f\"Random Forest - R² Score: {r2_rf}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting actual vs predicted values for Random Forest\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=y_test, y=y_pred_rf, color='blue')\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Random Forest - Actual vs Predicted Values')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**More libraries to be import**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the target variable (dependent variable)\ntarget2 = 'Net profit'\n\n# Define the features (independent variables)\nfeatures2 = ['Trades', 'Gross profit', 'Gross loss', '% change', 'Profit factor', 'Winners']\n\n# Create feature matrix (X) and target vector (y)\nX = data[features2]\ny = data[target2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform the features for the entire dataset\nX_scaled = scaler.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Neural Network Model**","metadata":{}},{"cell_type":"code","source":"# Initialize the Neural Network model\nmodel = Sequential()\n\n# Add input layer (input_dim should match the number of features)\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n\n# Add hidden layers\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\n\n# Add output layer (since this is regression, no activation function is used here)\nmodel.add(Dense(1))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the testing data\ny_pred_nn = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred_nn)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred_nn)\n\n# Calculate R-squared (R²)\nr2 = r2_score(y_test, y_pred_nn)\n\n# Display the evaluation metrics\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared: {r2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plot of actual vs predicted values\nplt.scatter(y_test, y_pred_nn)\nplt.xlabel('Actual Net Profit')\nplt.ylabel('Predicted Net Profit')\nplt.title('Neural Network - Actual vs Predicted Net Profit')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**KNN Regressor Model**","metadata":{}},{"cell_type":"code","source":"# Initialize the KNN Regressor model with a specific number of neighbors (e.g., 5)\nknn_model = KNeighborsRegressor(n_neighbors=5)\n\n# Train the model on the training data\nknn_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the testing data\ny_pred_knn = knn_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred_knn)\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred_knn)\n\n# Calculate R-squared (R²)\nr2 = r2_score(y_test, y_pred_knn)\n\n# Display the evaluation metrics\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared: {r2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example: Tuning the number of neighbors\nfor k in range(1, 11):\n    knn_model = KNeighborsRegressor(n_neighbors=k)\n    knn_model.fit(X_train, y_train)\n    y_pred = knn_model.predict(X_test)\n    r2 = r2_score(y_test, y_pred_knn)\n    print(f\"Number of Neighbors: {k}, R-squared: {r2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plot of actual vs predicted values\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Net Profit')\nplt.ylabel('Predicted Net Profit')\nplt.title('KNN Regressor Model - Actual vs Predicted Net Profit')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation results dictionary to store MSE and R² for each model\nresults = {\n    'Model': ['KNN Regressor', 'Linear Regression', 'Decision Tree Regressor', 'Random Forest Regressor', 'Neural Network Regressor'],\n    'MSE': [],\n    'R²': []\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to evaluate and store the results in the dictionary\ndef evaluate_model(name, y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    results['MSE'].append(mse)\n    results['R²'].append(r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate each model\nevaluate_model('KNN Regressor', y_test, y_pred_knn)\nevaluate_model('Linear Regression', y_test, y_pred_lr)\nevaluate_model('Decision Tree Regressor', y_test, y_pred_dt)\nevaluate_model('Random Forest Regressor', y_test, y_pred_rf)\nevaluate_model('Neural Network Regressor', y_test, y_pred_nn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert results dictionary to DataFrame for easier plotting\nresults_df = pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Bar Chart for R² ###\nplt.figure(figsize=(10, 6))\nplt.barh(results_df['Model'], results_df['R²'], color=['blue', 'green', 'orange', 'red', 'purple'])\nplt.xlabel('R-squared (R²)')\nplt.title('Model Comparison - R²')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Challenges Faced and Problem-Solving Approach**","metadata":{}},{"cell_type":"markdown","source":"**1. Data Preparation Challenges**\n\nThe data is collected for nearly 1 year and is still running. Standardized the features using StandardScaler for models like KNN and Neural Networks, which are sensitive to feature scaling.\n\n**2. Model Selection and Tuning**\n\n**Challenge:** \nChoosing the appropriate machine learning models and finding the right hyperparameters to ensure accurate predictions.\n\n**Approach:**\nSelected a diverse set of models, each representing a different type of machine learning algorithm (e.g., Linear Regression, KNN, Decision Tree, Neural Network, and Random Forest).\nUsed GridSearchCV for hyperparameter tuning to optimize models like KNN, Decision Tree, and Random Forest.\nCarefully tuned hyperparameters such as n_neighbors in KNN and max_depth in Decision Tree and Random Forest.\n\n**3. Model Comparison and Evaluation**\n\n**Challenge:** \nComparing different models with various strengths and weaknesses required careful evaluation using appropriate metrics.\n\n**Approach:**\nUsed Mean Squared Error (MSE) and R-squared (R²) to compare the models’ performances. These metrics were chosen because they provide insight into prediction error and the percentage of variance explained by the model.\nVisualized the results using bar charts to clearly highlight the differences in model performance.\n\n**4. Neural Network Configuration**\n\n**Challenge:**\nConfiguring a neural network that could effectively learn from the data was more complex compared to traditional models.\n\n**Approach:**\nDesigned a feed-forward neural network using Keras, with hidden layers and a ReLU activation function, which is commonly used for regression tasks.\nExperimented with the number of neurons, layers, and epochs to balance model complexity with performance.\nUsed StandardScaler to ensure that the input data was scaled appropriately for the neural network.\n\n**5. Handling Non-Linearities and Complex Patterns**\n\n**Challenge:**\nSome models like Linear Regression might not capture complex relationships in the data.\n\n**Approach:**\nIncorporated models like Random Forest and Neural Networks that can handle non-linear patterns and interactions between features.\nDecision Trees and Random Forest can automatically capture non-linearities, while the Neural Network model can learn more complex patterns with multiple layers.\n\n**6. Computational Cost**\n\n**Challenge:** \nTraining models like Random Forest and Neural Networks on larger datasets can be computationally expensive and time-consuming.\n\n**Approach:**\nUsed a sample of the dataset during the initial model training and testing phase to reduce computation time.\nOptimized the number of trees and depth in the Random Forest model to balance performance with speed.\nLimited the number of epochs for Neural Networks during hyperparameter tuning, then fine-tuned the model using a higher number of epochs once the architecture was optimized.\n\n**7. Model Interpretability**\n\n**Challenge:** \nInterpreting complex models like Neural Networks and Random Forest can be difficult compared to more straightforward models like Linear Regression.\n\n**Approach:**\nFor Random Forest, used feature importance scores to understand which features contributed the most to the model’s predictions.\nFor Neural Networks, explored the use of advanced techniques such as SHAP (SHapley Additive exPlanations) for better model interpretability, though this was left as a potential next step.","metadata":{}},{"cell_type":"markdown","source":"**To improve the overall performance and robustness of your machine learning models**","metadata":{}},{"cell_type":"markdown","source":"**1. Enhancing Data Quality**\n\n**Feature Engineering**\n\n- Explore feature interactions that might be relevant, especially for non-linear models like Random Forest or Neural Networks.\n- Use domain knowledge to identify potentially useful features that may not be directly available in the raw dataset.\n\n**2. Model Improvement**\n\n**Ensemble Learning**\n\n- Use Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost) to further boost performance, as these algorithms often outperform individual models on structured/tabular data.\n\n**Neural Network Architecture Optimization**\n\nDeep learning models can be optimized further with proper architecture design and tuning.\n\n- Experiment with different neural network architectures (e.g., more layers, wider layers, different activation functions).\n- Implement dropout layers and batch normalization to prevent overfitting and speed up training.\n- Tune hyperparameters like the learning rate, batch size, and number of neurons in each layer using a tool like Keras Tuner or Optuna.\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}